{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caderno para construção e teste do BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loader do PDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "path = \"data\\\\planejamento_estrategico.pdf\"\n",
    "\n",
    "## Loaders structured and unstructured\n",
    "\n",
    "#loader_un = UnstructuredPDFLoader(path)\n",
    "loader_structured = PyMuPDFLoader(path)\n",
    "\n",
    "pages_str = loader_structured.load()\n",
    "#pages_str = loader_un.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "## Criando texto splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    "    )\n",
    "\n",
    "## Split\n",
    "texts = text_splitter.split_documents(pages_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planejamento Estratégico\\n2\\nPlanejamento Estratégico da Fhemig 2024-2027\\nMissão\\nDetalhamento da Visão\\nObjetivo da Visão\\n3. Valores\\nValores da Fhemig\\nExemplo de Aplicação dos Valores\\n4. Contextualização do Referencial Estratégico\\nVariações de Solicitação de Informações\\nResumo Rápido\\nIndicadores do Planejamento Estratégico da Fhemig 2024-2027\\nIndicadores e Metas para 2027\\n1. Taxa de Ocupação Hospitalar (TOH)\\n2. Tempo Médio de Permanência Hospitalar (TMP)\\n3. Número de Internações\\n4. Número de Cirurgias\\n5. Número de Doadores Efetivos\\nComo os Indicadores São Utilizados?\\nResumo\\nVariações de Solicitação de Informações\\nObjetivos Estratégicos da Fhemig 2024-2027\\n1. Perspectiva: Sociedade\\n2. Perspectiva: Processos\\n3. Perspectiva: Financeiro\\n4. Perspectiva: Aprendizado e Crescimento\\nResumo\\nVariações de Solicitação de Informações\\nQ&A\\nMissão, Visão e Valores\\nObjetivos Estratégicos\\nIndicadores e Metas\\nMetodologia Utilizada\\nIniciativas e Projetos Estratégicos\\nContextualizações e Exemplos Práticos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testanto\n",
    "\n",
    "texts[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para remover espaço em branco\n",
    "\n",
    "def remove_ws(d):\n",
    "    text = d.page_content.replace(\"\\n\", \" \")\n",
    "    d.page_content = text\n",
    "    return d\n",
    "\n",
    "# Clean texts by removing whitespace from each document\n",
    "texts_cleaned = [remove_ws(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## openai\n",
    "\n",
    "# config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar as variáveis de ambiente do arquivo .env\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "\n",
    "# Acessar a chave da OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_23408\\1278828396.py:8: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "## retriever openai\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# uses OpenAI embeddings to build a retriever\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "# Creates the document retriever using docs and embeddings\n",
    "db = FAISS.from_documents(texts_cleaned, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# Asking the retriever to do similarity search based on Query\n",
    "#query = \"Foreign Aid for Lowari Road Tunnel & Access Roads Project (2nd Revised )\"\n",
    "#answer = db.similarity_search(query)\n",
    "\n",
    "# Building the retriever\n",
    "retriever = db.as_retriever(search_kwargs={'k': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Salvar o índice FAISS\n",
    "faiss.write_index(db.index, 'data/faiss_index.index')\n",
    "\n",
    "# Salvar os documentos associados (metadados)\n",
    "with open('data/faiss_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(db.docstore._dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() missing 1 required positional argument: 'index_to_docstore_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Recriar o FAISS VectorStore usando o índice e os documentos\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Recriar o retriever\u001b[39;00m\n\u001b[0;32m     20\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.__init__() missing 1 required positional argument: 'index_to_docstore_id'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Carregar o índice FAISS\n",
    "index = faiss.read_index('data/faiss_index.index')\n",
    "\n",
    "# Carregar os documentos (metadados)\n",
    "with open('data/faiss_docs.pkl', 'rb') as f:\n",
    "    docstore = pickle.load(f)\n",
    "\n",
    "# Recriar o OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Recriar o FAISS VectorStore usando o índice e os documentos\n",
    "db = FAISS(embedding_function=embeddings, index=index, docstore=docstore)\n",
    "\n",
    "# Recriar o retriever\n",
    "retriever = db.as_retriever(search_kwargs={'k': 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain and chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Função que o bot usará para responder a mensagens diretas\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "ara treinar o modelo no contexto do seu projeto de chatbot com RAG, considerando as melhores práticas para instruir o modelo com precisão:\n",
    "\n",
    "plaintext\n",
    "Copiar código\n",
    "Você é um chatbot especializado em responder perguntas sobre o Planejamento Estratégico da Fhemig 2024-2027. Sua base de conhecimento está organizada em módulos como Missão, Visão, Valores, Objetivos Estratégicos, Indicadores e Iniciativas. Use as informações fornecidas no {context} para responder de forma assertiva, clara e objetiva. Se não encontrar informações relevantes no contexto, informe que não pode responder e sugira que o usuário refine sua pergunta.\n",
    "\n",
    "### Contexto:\n",
    "{context}\n",
    "\n",
    "### Instruções:\n",
    "1. Leia o contexto acima para obter informações necessárias para responder.\n",
    "2. Responda de forma clara, usando exemplos práticos se aplicável.\n",
    "3. Se a pergunta estiver ambígua ou incompleta, peça esclarecimentos.\n",
    "4. Nunca invente informações que não estejam no {context}.\n",
    "5. Ofereça respostas em linguagem acessível, mas técnica quando necessário.\n",
    "\n",
    "### Formato da Resposta:\n",
    "1. Comece respondendo diretamente à pergunta.\n",
    "2. Inclua exemplos ou explicações adicionais para enriquecer a resposta.\n",
    "3. Caso relevante, apresente conexões com outras áreas do Planejamento Estratégico.\n",
    "\n",
    "### Exemplos de perguntas:\n",
    "- Qual é a missão da Fhemig?\n",
    "- Como o Planejamento Estratégico impacta a experiência do usuário?\n",
    "- Qual é a meta para a Taxa de Ocupação Hospitalar?\n",
    "- O que significa \"fortalecer a governança corporativa\" no contexto da Fhemig?\n",
    "\n",
    "Agora, responda à pergunta fornecida pelo usuário:\n",
    "\n",
    "PERGUNTA DO USUÁRIO: {pergunta}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Converter o template em prompt_template\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "## Configurar llm com Ollama\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "## Configurar chain\n",
    "\n",
    "chain = chain = (\n",
    "# The initial dictionary uses the retriever and user supplied query\n",
    "    {\"context\":retriever,\n",
    "     \"pergunta\":RunnablePassthrough()}\n",
    "# Feeds that context and query into the prompt then model & lastly \n",
    "# uses the ouput parser, do query for the data.\n",
    "    |  prompt  | llm | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As metas da Fhemig no Planejamento Estratégico 2024-2027 estão organizadas em objetivos estratégicos que orientam suas ações para alcançar a visão de ser reconhecida pela eficiência na gestão de serviços de saúde pública. Esses objetivos estão divididos em quatro perspectivas principais, seguindo a metodologia do Balanced Scorecard (BSC):\n",
      "\n",
      "\n",
      "\n",
      "1. **Perspectiva Sociedade:**\n",
      "\n",
      "   - Assegurar à sociedade serviços tempestivos e de qualidade.\n",
      "\n",
      "   - Melhorar a experiência e satisfação do usuário.\n",
      "\n",
      "   - Aumentar o acesso dos usuários aos serviços prestados.\n",
      "\n",
      "\n",
      "\n",
      "2. **Perspectiva de Aprendizado e Crescimento:**\n",
      "\n",
      "   - Criação de uma política de remuneração atrelada a resultados e produtividade.\n",
      "\n",
      "   - Programas de saúde ocupacional para garantir a segurança e bem-estar dos colaboradores.\n",
      "\n",
      "\n",
      "\n",
      "Para atingir essas metas, a Fhemig utiliza indicadores mensuráveis, como indicadores de qualidade do atendimento ao usuário e indicadores financeiros, para monitorar o progresso e corrigir rotas quando necessário.\n",
      "\n",
      "\n",
      "\n",
      "Se precisar de mais informações ou detalhes específicos sobre cada meta, por favor, forneça mais detalhes para que eu possa ajudar melhor.\n"
     ]
    }
   ],
   "source": [
    "input = \"Quais são as metas da Fhemig?\"\n",
    "\n",
    "\n",
    "response = chain.invoke(input)\n",
    "formatted_response = response.replace(\"\\n\", \"\\n\\n\")  # Adiciona uma linha em branco após cada nova linha\n",
    "\n",
    "print(formatted_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chatbots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
