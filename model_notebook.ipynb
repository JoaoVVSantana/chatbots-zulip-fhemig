{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caderno para construção e teste do BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loader do PDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "path = \"data\\\\manual_consolidado.pdf\"\n",
    "\n",
    "## Loaders structured and unstructured\n",
    "\n",
    "#loader_un = UnstructuredPDFLoader(path)\n",
    "loader_structured = PyMuPDFLoader(path)\n",
    "\n",
    "pages_str = loader_structured.load()\n",
    "#pages_str = loader_un.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "## Criando texto splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    "    )\n",
    "\n",
    "## Split\n",
    "texts = text_splitter.split_documents(pages_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O sistema gera uma confirmação da solicitação, clicar em “Ok”.\\nO status final do processo fica como “Lote entregue”. Após a finalização, o processo não pode\\nmais ser excluído e o botão “Excluir” fica indisponível.\\nO novo lote pode ser consultado na aba “Lote fornecedor”, com uma nova sequência que\\nfoi criada. O lote que deu origem ao processo de unitarização segue com o status\\n“Enviado para unitarização”.\\nConsultores:\\nBruno Emanoel Calixto Ferreira\\nUllysses Henrique Machado Ribas\\nPágina:\\n75 de 80'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testanto\n",
    "\n",
    "texts[500].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para remover espaço em branco\n",
    "\n",
    "def remove_ws(d):\n",
    "    text = d.page_content.replace(\"\\n\", \"\")\n",
    "    d.page_content = text\n",
    "    return d\n",
    "\n",
    "# Clean texts by removing whitespace from each document\n",
    "texts_cleaned = [remove_ws(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5458, 5458)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_cleaned[500].page_content\n",
    "\n",
    "len(texts), len(texts_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import joblib\n",
    "\n",
    "## Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2:1b\")\n",
    "\n",
    "# Create the vectorstore\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=texts_cleaned,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "[]\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Salvar o retriever no arquivo\n",
    "joblib.dump(retriever, 'data\\\\retriever.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salvando retriever\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Criar e salvar o retriever\n",
    "retriever = ...  # O código que cria o retriever a partir do PDF dos manuais\n",
    "\n",
    "# Salvar o retriever no arquivo\n",
    "\n",
    "\n",
    "joblib.dump(retriever, 'data\\\\retriever.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregando retriever\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Carregar o retriever salvo\n",
    "retriever = joblib.load('data\\\\retriever.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain and chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m\n\u001b[0;32m     71\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2:1b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m## Configurar chain\u001b[39;00m\n\u001b[0;32m     75\u001b[0m chain \u001b[38;5;241m=\u001b[39m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# The initial dictionary uses the retriever and user supplied query\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mretriever\u001b[49m,\n\u001b[0;32m     78\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m:RunnablePassthrough()}\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Feeds that context and query into the prompt then model & lastly \u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# uses the ouput parser, do query for the data.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;241m|\u001b[39m  prompt  \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     82\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Função que o bot usará para responder a mensagens diretas\n",
    "\n",
    "template = \"\"\"\n",
    "Você é um assistente especializado no sistema de gestão hospitalar Tasy, utilizado pela Fundação Hospitalar do Estado de Minas Gerais (FHEMIG).\n",
    "Seu objetivo é ajudar os usuários a utilizarem o Tasy de forma eficiente, respondendo a perguntas com base nos manuais e documentos internos disponíveis.\n",
    "\n",
    "Além disso, você utiliza a técnica de **Chain of Thought (CoT)** para resolver problemas mais complexos. Isso significa que você deve fornecer um raciocínio passo a passo para as questões que exigem várias etapas ou decisões. Explicite seu processo de raciocínio antes de chegar à resposta final.\n",
    "\n",
    "### Instruções:\n",
    "\n",
    "1. Sempre forneça uma resposta clara e objetiva com base nas informações extraídas dos manuais.\n",
    "2. Utilize um raciocínio passo a passo (Chain of Thought) para resolver perguntas complexas ou processos que envolvem múltiplas etapas.\n",
    "3. Se necessário, você pode buscar em múltiplos documentos para encontrar a melhor resposta.\n",
    "4. Caso a resposta seja longa ou envolva várias etapas, quebre-a em passos claros e numerados.\n",
    "5. Se o usuário pedir mais detalhes, você pode expandir a explicação ou oferecer links para seções específicas dos manuais.\n",
    "\n",
    "### Estrutura de Resposta:\n",
    "1. **Raciocínio Passo a Passo (Chain of Thought)**: Para perguntas que envolvem várias etapas ou decisões, forneça um raciocínio lógico que explique o processo antes de chegar à resposta final.\n",
    "2. **Resumo Rápido**: Forneça uma visão geral da resposta em uma ou duas frases.\n",
    "3. **Passos Detalhados** (se aplicável): Se o problema envolver etapas, forneça uma lista numerada.\n",
    "4. **Referência ao Manual**: Sempre que possível, inclua referências ao manual, como o número da página ou seção relevante.\n",
    "\n",
    "### Exemplos de Perguntas e Respostas:\n",
    "\n",
    "Exemplo 1:\n",
    "**Pergunta**: Como faço para gerar um relatório de pacientes?\n",
    "**Resposta**:\n",
    "\n",
    "1. **Chain of Thought**: Para gerar um relatório de pacientes no Tasy, precisamos entender qual módulo acessar, como configurar os filtros, e como gerar o relatório final. O sistema Tasy exige que o usuário escolha os filtros adequados para garantir que o relatório seja útil. Vamos passo a passo:\n",
    "   - Primeiro, identifique o módulo correto (Relatórios).\n",
    "   - Em seguida, selecione o tipo de relatório adequado (Relatório de Pacientes).\n",
    "   - Depois, configure os filtros, como data, tipo de paciente, etc.\n",
    "   - Finalmente, clique em \"Gerar\" para visualizar o relatório.\n",
    "\n",
    "2. **Resumo Rápido**: Para gerar um relatório de pacientes, você deve acessar o módulo \"Relatórios\" e selecionar \"Relatório de Pacientes\".\n",
    "3. **Passos Detalhados**: \n",
    "   - Vá para o menu principal.\n",
    "   - Selecione \"Relatórios\".\n",
    "   - Escolha \"Relatório de Pacientes\".\n",
    "   - Defina os filtros necessários, como período ou tipo de paciente.\n",
    "   - Clique em \"Gerar\".\n",
    "4. **Referência ao Manual**: Consulte o manual do Tasy, página 235, para mais informações detalhadas sobre os filtros aplicáveis.\n",
    "\n",
    "Exemplo 2:\n",
    "**Pergunta**: Qual a função do campo \"Status\" no cadastro de pacientes?\n",
    "**Resposta**:\n",
    "\n",
    "1. **Chain of Thought**: O campo \"Status\" no cadastro de pacientes tem várias funções, incluindo o rastreamento da situação do paciente. Podemos começar compreendendo o que cada status representa. Por exemplo, um paciente ativo pode estar atualmente recebendo tratamento, enquanto um paciente inativo pode ter sido desligado do sistema. Vamos analisar como esse campo impacta a gestão de pacientes.\n",
    "   - O campo \"Status\" pode ser configurado para refletir o estado do paciente.\n",
    "   - Ele também pode ser utilizado por administradores para definir o acesso a determinados módulos do sistema.\n",
    "\n",
    "2. **Resumo Rápido**: O campo \"Status\" indica a situação atual do paciente no sistema.\n",
    "3. **Passos Detalhados**: Ele pode ser utilizado para identificar se o paciente está ativo, inativo ou internado. Pode ser atualizado pelo administrador.\n",
    "4. **Referência ao Manual**: Ver manual do Tasy, seção 3.5, página 47.\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Converter o template em prompt_template\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "## Configurar llm com Ollama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "## Configurar chain\n",
    "\n",
    "chain = chain = (\n",
    "# The initial dictionary uses the retriever and user supplied query\n",
    "    {\"context\":retriever,\n",
    "     \"query\":RunnablePassthrough()}\n",
    "# Feeds that context and query into the prompt then model & lastly \n",
    "# uses the ouput parser, do query for the data.\n",
    "    |  prompt  | llm | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"input\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chatbots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
